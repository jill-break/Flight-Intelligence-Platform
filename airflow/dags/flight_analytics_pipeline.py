"""
Flight Analytics Pipeline DAG â€” Processing Engine
====================================================
Waits for files in MinIO â†’ Validates with Pandera â†’ Transforms with Spark â†’ Archives

This DAG is DECOUPLED from ingestion â€” it only processes files already in MinIO.
Files arrive via the raw_ingestion_pipeline DAG (local uploads) or other sources.

Task flow:
  wait_for_file â†’ validate_with_pandera â†’ process_flight_data_spark â†’ archive_processed_files

Data Quality Gate:
  - Pandera validates every CSV against FlightSchema (lazy=True)
  - If ANY file fails â†’ DAG fails, Spark never runs
  - Bad files are quarantined to quarantine/ in MinIO
"""

from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.exceptions import AirflowFailException
from datetime import datetime, timedelta
import pandas as pd
import pandera
import io

from spark.schemas.flight_schema import FlightSchema

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Default Args
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
default_args = {
    'owner': 'senior_de',
    'depends_on_past': False,
    'start_date': datetime(2026, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email': ['data-team@flightintel.io'],
}

MINIO_CONN_ID = 'minio_conn'
BUCKET = 'raw-data'


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Task: Data Quality Gate (Pandera)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_flight_data(**kwargs):
    """
    Validate all new CSVs against the Pandera FlightSchema.
    - Valid files   â†’ moved to validated/
    - Invalid files â†’ moved to quarantine/
    - If ANY file fails â†’ raise AirflowFailException to block Spark
    """
    s3_hook = S3Hook(aws_conn_id=MINIO_CONN_ID)

    keys = s3_hook.list_keys(bucket_name=BUCKET, prefix='flights_')
    if not keys:
        print("No new files to validate.")
        return

    failed_files = []
    validated_files = []

    for key in keys:
        file_obj = s3_hook.get_key(key, BUCKET)
        file_content = file_obj.get()['Body'].read().decode('utf-8')
        df = pd.read_csv(io.StringIO(file_content))

        try:
            # lazy=True collects ALL schema errors before raising
            FlightSchema.validate(df, lazy=True)
            print(f"âœ… Validation PASSED: {key}")

            # Move to validated/
            new_key = f"validated/{key.split('/')[-1]}"
            s3_hook.copy_object(
                source_bucket_key=key, dest_bucket_key=new_key,
                source_bucket_name=BUCKET, dest_bucket_name=BUCKET
            )
            s3_hook.delete_objects(BUCKET, key)
            validated_files.append(key)

        except pandera.errors.SchemaErrors as e:
            error_count = len(e.failure_cases)
            print(f"âŒ Validation FAILED: {key} â€” {error_count} errors found")
            print(e.failure_cases.to_string())

            # Quarantine bad data
            new_key = f"quarantine/{key.split('/')[-1]}"
            s3_hook.copy_object(
                source_bucket_key=key, dest_bucket_key=new_key,
                source_bucket_name=BUCKET, dest_bucket_name=BUCKET
            )
            s3_hook.delete_objects(BUCKET, key)
            failed_files.append(key)

    # DATA QUALITY GATE: Block downstream tasks if any file failed
    if failed_files:
        raise AirflowFailException(
            f"Data quality check FAILED. "
            f"{len(failed_files)} file(s) quarantined: {failed_files}. "
            f"Spark processing blocked."
        )

    print(f"All {len(validated_files)} file(s) passed validation.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Task: Automated Archive Cleanup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def archive_processed_files(**kwargs):
    """
    Move files from validated/ to archived/{date}/ after Spark processes them.
    Prevents files from being reprocessed on the next DAG run.
    """
    s3_hook = S3Hook(aws_conn_id=MINIO_CONN_ID)
    today = datetime.now().strftime('%Y-%m-%d')

    keys = s3_hook.list_keys(bucket_name=BUCKET, prefix='validated/')
    if not keys:
        print("No files to archive.")
        return

    archived_count = 0
    for key in keys:
        filename = key.split('/')[-1]
        if not filename:
            continue

        archive_key = f"archived/{today}/{filename}"
        s3_hook.copy_object(
            source_bucket_key=key, dest_bucket_key=archive_key,
            source_bucket_name=BUCKET, dest_bucket_name=BUCKET
        )
        s3_hook.delete_objects(BUCKET, key)
        archived_count += 1
        print(f"ðŸ“¦ Archived: {key} â†’ {archive_key}")

    print(f"Archived {archived_count} file(s) to archived/{today}/")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DAG Definition
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with DAG(
    'flight_analytics_pipeline',
    default_args=default_args,
    description='Processing Engine: Validate â†’ Spark Transform â†’ Archive',
    schedule_interval='@hourly',
    catchup=False,
    tags=['analytics', 'processing', 'flight-data'],
) as dag:

    # 1. Wait for new flight data in MinIO
    wait_for_file = S3KeySensor(
        task_id='wait_for_flight_data',
        bucket_name=BUCKET,
        bucket_key='flights_*.csv',
        wildcard_match=True,
        aws_conn_id=MINIO_CONN_ID,
        timeout=18 * 60 * 60,
        poke_interval=30,
    )

    # 2. Data Quality Gate â€” Pandera validation
    validate_data = PythonOperator(
        task_id='validate_with_pandera',
        python_callable=validate_flight_data,
    )

    # 3. Spark Processing (Transform & Load to Postgres)
    process_data = SparkSubmitOperator(
        task_id='process_flight_data_spark',
        application='/opt/airflow/spark/jobs/process_flights.py',
        conn_id='spark_default',
        jars='/opt/airflow/spark/jars/postgresql-42.7.6.jar',
        packages='org.apache.hadoop:hadoop-aws:3.3.4',
        conf={
            'spark.driver.host': 'airflow-scheduler',
            'spark.driver.bindAddress': '0.0.0.0',
            'spark.hadoop.fs.s3a.endpoint': 'http://minio:9000',
            'spark.hadoop.fs.s3a.access.key': 'minio_admin',
            'spark.hadoop.fs.s3a.secret.key': 'minio_password_321',
            'spark.hadoop.fs.s3a.path.style.access': 'true',
            'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
        },
        verbose=True,
    )

    # 4. Archive processed files so they aren't reprocessed
    archive_files = PythonOperator(
        task_id='archive_processed_files',
        python_callable=archive_processed_files,
    )

    # Pipeline flow
    wait_for_file >> validate_data >> process_data >> archive_files
